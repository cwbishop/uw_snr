---
title: 'SNR: Behavioral Results'
author: "Christopher W. Bishop"
output:
  html_document:
    toc: yes
  pdf_document:
    toc: yes
    toc_depth: 3
---

## Overview

Michael Lee provided a preliminary pass of HINT data from UW/UofI. Here is a quick and dirty breakdown of the data.

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
                      echo=FALSE, warning=FALSE, message=FALSE)
```
```{r}
# Preamble to load libraries, massage data
library(readr)
library(dplyr)
library(ggplot2)

# Load in data
hint_default = read_csv('Default_calculation_batch_HINT.csv')
hint_reversal = read_csv('Reversal_mean_calculation_batch_HINT.csv')

# Coerce subject IDs to strings
hint_default = hint_default %>% mutate(SubID = as.character(SubID))
hint_reversal = hint_reversal %>% mutate(SubID = as.character(SubID))

# Add in site codes
site_code = unlist( (hint_default %>% rowwise %>% do(site=if(.$SubID<2000){'UW'}else{'Iowa'}))$site )
hint_default = hint_default %>% mutate(site = site_code)

# Repeat for hint_reversal
site_code = unlist( (hint_reversal %>% rowwise %>% do(site=if(.$SubID<2000){'UW'}else{'Iowa'}))$site )
hint_reversal = hint_reversal %>% mutate(site = site_code)

# Function to plot each subject's data, facet by noise type
plot.measure <- function(df_table, var){
  "
  Plot variable of interest for each subscriber by site/noise type
  "
  
  plot = ggplot(df_table %>% mutate(SubID=as.character(SubID)), aes_string(x='SubID', y=var, fill='site')) + 
          geom_bar(stat='identity') + 
          theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
          facet_wrap(~noise_type, ncol=1)
  
  return(plot)
  
}

summarise.table <- function(df_table){
  "
  Summarises tables with mean/sd
  "
  table_summary = df_table %>% 
                  group_by(site, noise_type) %>% 
                  summarise_each(funs(mean(., na.rm=TRUE), sd(., na.rm=TRUE)), matches('mean|slope'))
  
  return(table_summary)
  
}

plot.summary <- function(df_table, var){
  "
  Quick and dirty summary plots
  "
  
  # Get table summaries
  table_summary = summarise.table(df_table)
  
  # Create plots with table summaries
  plot = ggplot(table_summary, aes_string(x='noise_type', y=paste0(var, '_mean'), fill='site')) + 
         geom_bar(position='dodge', stat='identity')
         # geom_errorbar(aes_string(ymin=paste0(var, '_mean') - paste0(var, '_sd'), ymax=paste0(var, '_mean')+paste0(var, '_sd')), position='dodge')
  
  return(plot)
}
```

### Default Scoring

Below are subject-by-subject data points from UW/UofI using the default scoring parameters.

**Note**: U of I has twice as many subjects as UW, so interpretations will need to be coloured accordingly.

```{r}
print(plot.measure(hint_default, 'mean_SNR50'))
print(plot.measure(hint_default, 'mean_SNR80'))
print(plot.measure(hint_default, 'slope'))
```

- Qualitatively, there seem to be intersite differences, particularly with SPSHN SNR80/SNR50 estimates.
- There are slope outliers we'll need to look into and deal with accordingly. Particularly interested in ISTS from Iowa.

**Michael**: Can you please provide some detail as to how you calculated slope_dB? I know we've decided to throw that measure out, but I want to make sure we are calculating the slope appropriately ... and I'm not sure how slope and slope_dB can differ as much as they do. And And sometimes (e.g., 1014), the two measures differ in sign.

Below are summary figures (no statistics yet) for each of the three measures plotted in detail above.

```{r}
# Plot out summaries by variable
print(plot.summary(hint_default, 'mean_SNR50'))
print(plot.summary(hint_default, 'mean_SNR80'))
print(plot.summary(hint_default, 'slope'))
```

- These averages should be interpretted with considerable caution. I have done *nothing* to deal with outliers or impute missing data intelligently. Too tired to do either, at the moment.
    - Regardless, we are seeing some reasonably large intersite differences in our HINT scoring. Need to look into this.

### Reversal Scoring

Below, SNR-50 and SNR-80 calculations are both scored using the mean of N reversals.

**Michael**: can you please confirm how many reversals we are using for scoring and their relative positions (e.g., reversals 2-6)? Do all subjects have all reversals? I seem to recall we adapted the SNR-50 protocol later in the game, so I wouldn't expect all subjects to have enough reversals unless we were extremely lucky. Or I'm very tired. One of those is a certainty.

Below are subject-wise plots of SNR50/80/and slope.


```{r}
print(plot.measure(hint_reversal, 'mean_SNR50'))
print(plot.measure(hint_reversal, 'mean_SNR80'))
print(plot.measure(hint_reversal, 'slope'))
```

- Again, we're seeing outliers in slope, but they're far less extreme it seems.
    - Outliers include 
- **Michael** can you cross-check these figures with those above? Are the outliers the same subjects?

Below are the measure summaries using reversal scoring.

```{r}
# Plot out summaries by variable
print(plot.summary(hint_default, 'mean_SNR50'))
print(plot.summary(hint_default, 'mean_SNR80'))
print(plot.summary(hint_default, 'slope'))
```

- We're still seeing intersite differences, with generally higher SNRs at UW than Iowa.

**Anyone have any thoughts on why this might be?**

\pagebreak

## Logistic Regression

As shown above, there are several subjects whose slope estimates are hyper-inflated. This is due to a small difference in SNR-50/SNR-80 within a given condition. These estimates are inherently noisy and narrowing our analyses to a handful of datapoints (of tens) may be hurting us.

One potential remedy to this problem is to fit *all* data points within a single subject and masker with a logistic curve. The general idea is to leverage all of the data we have at our disposal to reconstruct full psychometric functions rather than using reversals to target specific points along the psychometric functions.

### Approach

The trial-to-trial SNR values are loaded for both the SNR-50 and SNR-80 trials for each subject. Trials are then assigned a score of 0 or 1 depending on if the listener got all of the sentence correct or not. These data are then submitted to a GLM to estimate the overall psychometric functions.

- This process can likely be improved by using a psychometric function with a free parameter describing the slope of the psychometric function.
    - CWB has seen these many times before, but he does not have access to the papers at the moment.
    
```{r}
library(readr)
library(dplyr)
library(stringr)
library(foreach)
library(ggplot2)

setwd('~/Documents/GitHub/uw_snr/')

# Read in data
file = 'batch_HINT_TimeSeries.csv'


# Reshape into a more useful format
import.hint.ts <- function(file){
  "
  Read in HINT time series data and fill massage it into a more useful format
  "
  raw = read_csv(file)
  
  
  hint_ts <- foreach (i=seq(1, nrow(raw)), .combine=rbind) %do% {
    
    # Get temporary data frame
    tmp = raw[i,]
    
    # Split test_id
    test_id = str_split(tmp$Test_ID, '_')[[1]]
    
    # Get time series
    series = str_replace_all(tmp$Time_Series, '\\[|\\]', '') %>% str_split(';') %>% unlist() %>% as.integer()
    
    # New data frame
    return(data.frame(sub_id=tmp$SubID, test_id=test_id[2], masker=test_id[3], snr=series, trial=seq(1,length(series))))
    
  }
  
  # Score each subject/test
  hint_ts = 
    hint_ts %>%
    filter(!is.na(snr)) %>%
    group_by(sub_id, test_id, masker) %>% 
    mutate(delta = lead(snr) - snr) %>% 
    mutate(score= ifelse(delta<=0, 100, 0))
  
  return(hint_ts)
  
}

# Get scored data
hint_ts = import.hint.ts(file)

# Construct logistic regression models
models = 
  hint_ts %>%
  group_by(sub_id, masker) %>% # want a different fit for each subject/masker
  do(mod = glm(score/100 ~ snr, data=., family='binomial'))
```

```{r}
# Plot out all models and model fits
plots <- foreach (i=1:nrow(models)) %do%{
  
  # Get the current model
  mod = models[i,]
  
  # Need original data to get relevant SNR range
  snr =
    hint_ts %>%
    filter(sub_id==mod$sub_id, masker==mod$masker)
  
  # Domain
  x = seq(min(snr$snr), max(snr$snr),1) # 1 dB steps to start
  
  # Response
  y = predict(mod$mod[[1]], data.frame(snr=x), type='response')
  
  # Create plot
  ggplot(
    data.frame(x=x, y=y), 
    aes(x=x, y=y)) + geom_point(size=3) + # plot fit
    geom_line(size=2) +  
    geom_point(data=snr, aes(x=snr, y=score/100), colour='red', size=4, position=position_jitter(h=0.1, w=0.1)) + # plot original data points
    #geom_jitter(data=snr, aes(x=snr, y=score/100)) + # jitter plotted points so they aren't stacked on top of one another.
    ggtitle(paste0(mod$sub_id, '-', mod$masker)) + # put a title on it!
    ylab('Score') +
    xlab('SNR (dB)') +
    scale_x_continuous(breaks=seq(min(x), max(x), 1)) + # 1 dB ticks
    ylim(0, 1) + # show full axes
    scale_y_continuous(breaks=seq(0,1,0.05))
}
```

Now that we have a psychometric fit for each subject, we need to estimate SNR-50/80.

## To Do

- Rerun analyses and deal with outliers in an intelligent way.
    - Double check computations
- Combine with data from REDCAP!!
- Publish.